# -*- coding: utf-8 -*-
"""SearchJaroWinkler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jMp4mlOXlATt4lqkeokJm4kcfq-M-2Ra
"""

#pip install thefuzz[speedup]
#pip install JaroWinkler

import pandas as pd
import re
import numpy as np
import os
from thefuzz import fuzz
from rapidfuzz.distance import JaroWinkler



file_path = 'C:/Users/User/Downloads/individual_dataset (2).csv'
df = pd.read_csv(file_path)

df = df.drop(["SecondName", "ThirdName"], axis = 1)

df

import unicodedata

def preprocess_name(name):
    if isinstance(name, str):
        name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')
        name = re.sub(r'[^a-zA-Z\s]', '', name).lower().strip()
        return name
    return ''

df['FirstName'] = df['FirstName'].apply(preprocess_name)
df['LastName'] = df['LastName'].apply(preprocess_name)

df['FullName'] = df['FirstName'].fillna('') + ' ' + df['LastName'].fillna('')
df['FullName'] = df['FullName'].str.strip()

df

df = df.drop(["FirstName", "LastName", "IndividualAlias"], axis = 1)

df

df = df[df['FullName'].str.len() > 2]

# Define the search name and preprocess function
name_to_search = "AHMED IBRAHIM HUSSAIN ALKAF"

preprocessed_name = preprocess_name(name_to_search)

# Full Match Implementation
df['JaroWinkler'] = df['FullName'].apply(lambda name: JaroWinkler.similarity(preprocessed_name, name))

full_match_results = df[df['JaroWinkler'] > 0.95]

if not full_match_results.empty:
    print("Full Match Results:")
    print(full_match_results[["FullName", "JaroWinkler"]])
else:
    # Second Implementation (Partial Match with Tokenization)
    def tokenize_name(name):
        return name.split()

    def jaro_winkler_tokenized(name1, name2, print_scores=False):
        tokens1 = tokenize_name(name1)
        tokens2 = tokenize_name(name2)

        total_score = 0
        for token1 in tokens1:
            if tokens2:
                if print_scores:
                    # Print each token comparison score
                    for token2 in tokens2:
                        score = JaroWinkler.similarity(token1, token2)
                        print(f"Comparing '{token1}' with '{token2}': {score}")
                best_score = max(JaroWinkler.similarity(token1, token2) for token2 in tokens2)
                scaled_score = best_score **2   # Apply scaling (squared in this case)
                total_score += scaled_score

        return total_score / len(tokens1) if tokens1 else 0  # Average scaled score across tokens

    FullNames = df['FullName'].tolist()
    threshold = 0.8

    similar_names_jaro_winkler_combined = [(name, jaro_winkler_tokenized(preprocessed_name, name))
                                           for name in FullNames if jaro_winkler_tokenized(preprocessed_name, name) >= threshold]

    similar_names_jaro_winkler_combined = sorted(similar_names_jaro_winkler_combined, key=lambda x: x[1], reverse=True)

    # Print the results and token comparison scores for displayed results
    similar_names_jaro_winkler_combined_df = pd.DataFrame(similar_names_jaro_winkler_combined, columns=["Name", "Score"])

    print(similar_names_jaro_winkler_combined_df.head(40))

    # Print token comparison scores for the top 40 results
    # for name, score in similar_names_jaro_winkler_combined[:40]:
    #     print(f"\nToken comparison scores for: {name}")
    #     jaro_winkler_tokenized(preprocessed_name, name, print_scores=True)